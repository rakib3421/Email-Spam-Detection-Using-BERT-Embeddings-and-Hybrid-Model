
import streamlit as st
import joblib
import numpy as np
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk
import torch
from transformers import BertTokenizer, BertModel
import warnings
warnings.filterwarnings('ignore')

# Download NLTK resources
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)

# Page configuration
st.set_page_config(
    page_title="Direct Spam Detection",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .prediction-box {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
        text-align: center;
        font-size: 1.2rem;
        font-weight: bold;
    }
    .spam-result {
        background-color: #ffcccc;
        border: 2px solid #ff0000;
        color: #990000;
    }
    .ham-result {
        background-color: #ccffcc;
        border: 2px solid #00aa00;
        color: #006600;
    }
</style>
""", unsafe_allow_html=True)

# Title
st.markdown('<h1 class="main-header">üõ°Ô∏è Direct Spam Detection System</h1>', unsafe_allow_html=True)
st.markdown("### Powered by BERT + Hybrid ML Ensemble - Direct Predictions")
st.markdown("---")

# Sidebar
with st.sidebar:
    st.header("‚ÑπÔ∏è About")
    st.markdown("""
    This spam detection system provides **direct predictions** using:
    
    - **BERT Transformer** for deep text understanding
    - **TF-IDF** for traditional text features  
    - **Ensemble ML models** (SVM, Random Forest, XGBoost)
    - **Advanced preprocessing** with lemmatization
    - **Feature engineering** for spam indicators
    
    **Trained on:** Real email dataset with 5,854 emails
    **Mode:** Direct model prediction without adjustments
    """)
    
    st.header("üìä Model Performance")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Precision", "98.7%")
        st.metric("Recall", "97.7%")
    with col2:
        st.metric("F1-Score", "98.1%")
        st.metric("AUC", "99.96%")# Load the model
@st.cache_resource
def load_model():
    """Load the trained model components"""
    try:
        components = joblib.load('spam_detector_components.pkl')
        return components
    except FileNotFoundError:
        st.error("Model file not found! Please ensure 'spam_detector_components.pkl' is in the same directory.")
        return None

components = load_model()

if components is None:
    st.stop()

# Initialize detector
class DirectSpamDetector:
    """Direct spam detector - no context adjustments, pure model predictions"""
    
    def __init__(self, components):
        self.model = components['model']
        self.bert_model = components.get('bert_model')
        self.tokenizer = components.get('tokenizer')
        self.tfidf_vectorizer = components.get('tfidf_vectorizer')
        self.scaler = components.get('scaler')
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        if self.bert_model is not None:
            self.bert_model.to(self.device)
            self.bert_model.eval()
    
    def preprocess_text(self, text):
        """Preprocess text using the same logic as training"""
        if not isinstance(text, str):
            return ""
        
        text = text.lower()
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' url ', text)
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', ' email ', text)
        text = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', ' phone ', text)
        text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        try:
            tokens = word_tokenize(text)
            stop_words = set(stopwords.words('english'))
            tokens = [word for word in tokens if word not in stop_words and len(word) > 2]
            
            lemmatizer = WordNetLemmatizer()
            tokens = [lemmatizer.lemmatize(word) for word in tokens]
            
            return ' '.join(tokens)
        except:
            return text
    
    def extract_features(self, text):
        """Extract the same features used during training"""
        features = {
            'char_count': len(text),
            'word_count': len(text.split()),
            'sentence_count': len(re.findall(r'[.!?]+', text)),
            'avg_word_length': np.mean([len(word) for word in text.split()]) if text.split() else 0,
            'exclamation_count': text.count('!'),
            'question_count': text.count('?'),
            'dollar_count': text.count('$'),
            'caps_count': sum(1 for c in text if c.isupper()),
            'caps_ratio': sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0,
            'has_url': 1 if 'url' in text.lower() or 'http' in text.lower() else 0,
            'has_email': 1 if '@' in text else 0,
            'has_phone': 1 if re.search(r'\d{3}[-.]?\d{3}[-.]?\d{4}', text) else 0,
            'unique_word_ratio': len(set(text.split())) / len(text.split()) if text.split() else 0
        }
        
        # Spam indicators (same as training)
        spam_indicators = [
            'free', 'win', 'winner', 'cash', 'prize', 'urgent', 'limited', 'offer', 'deal', 'discount',
            'guarantee', 'money', 'credit', 'loan', 'investment', 'million', 'billion', 'rich', 'wealth',
            'congratulations', 'congrats', 'claim', 'click', 'subscribe', 'unsubscribe', 'buy', 'sale',
            'marketing', 'advertisement', 'promotion', 'newsletter', 'update', 'alert', 'notification'
        ]
        
        features['spam_word_count'] = sum(1 for word in spam_indicators if word in text.lower())
        features['spam_word_ratio'] = features['spam_word_count'] / features['word_count'] if features['word_count'] > 0 else 0
        
        return features
    
    def get_bert_embedding(self, text):
        """Get BERT embedding for text"""
        if self.bert_model is None or self.tokenizer is None:
            return np.zeros(768)  # Return zero embedding if BERT not available
        
        try:
            inputs = self.tokenizer(
                text,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
            
            return embedding
        except:
            return np.zeros(768)
    
    def predict(self, subject, message):
        """Make direct prediction using the trained model"""
        try:
            # Combine subject and message
            full_text = subject + ' ' + message
            processed_text = self.preprocess_text(full_text)
            
            # Extract all features (same as training)
            features = self.extract_features(full_text)
            
            # Get BERT embedding
            bert_embedding = self.get_bert_embedding(processed_text)
            
            # Get TF-IDF features
            if self.tfidf_vectorizer is not None:
                tfidf_features = self.tfidf_vectorizer.transform([processed_text]).toarray()[0]
            else:
                tfidf_features = np.zeros(5000)  # Default size
            
            # Get numerical features
            numerical_features = np.array([
                features['char_count'], features['word_count'], features['sentence_count'],
                features['avg_word_length'], features['exclamation_count'], features['question_count'],
                features['dollar_count'], features['caps_count'], features['caps_ratio'],
                features['spam_word_count'], features['spam_word_ratio'], features['has_url'],
                features['has_email'], features['has_phone'], features['unique_word_ratio']
            ])
            
            # Scale numerical features
            if self.scaler is not None:
                numerical_features_scaled = self.scaler.transform(numerical_features.reshape(1, -1))[0]
            else:
                numerical_features_scaled = numerical_features
            
            # Combine all features (same order as training)
            combined_features = np.concatenate([
                bert_embedding,
                tfidf_features,
                numerical_features_scaled
            ])
            
            # Make prediction using the trained model
            prediction = self.model.predict(combined_features.reshape(1, -1))[0]
            probabilities = self.model.predict_proba(combined_features.reshape(1, -1))[0]
            
            return {
                'prediction': 'spam' if prediction == 1 else 'ham',
                'spam_probability': float(probabilities[1]),
                'ham_probability': float(probabilities[0]),
                'confidence': float(max(probabilities)),
                'processed_text': processed_text,
                'features': features
            }
            
        except Exception as e:
            # Fallback prediction
            return {
                'prediction': 'ham',  # Conservative default
                'spam_probability': 0.0,
                'ham_probability': 1.0,
                'confidence': 0.5,
                'error': str(e),
                'processed_text': '',
                'features': {}
            }

detector = DirectSpamDetector(components)

# Main content
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("üìß Email Analysis")

    # Input fields
    subject = st.text_input("Email Subject:", placeholder="Enter email subject...")
    message = st.text_area("Email Message:", placeholder="Enter email message...", height=150)

    # Analysis button
    if st.button("üîç Analyze Email", type="primary", use_container_width=True):
        if subject.strip() or message.strip():
            with st.spinner("Analyzing email..."):
                result = detector.predict(subject, message)

            # Display result
            if result['prediction'] == 'spam':
                st.markdown('<div class="prediction-box spam-result">üö® SPAM DETECTED</div>', unsafe_allow_html=True)
            else:
                st.markdown('<div class="prediction-box ham-result">‚úÖ LEGITIMATE EMAIL</div>', unsafe_allow_html=True)

            # Confidence meter
            st.subheader("üìä Confidence Analysis")
            col_a, col_b = st.columns(2)

            with col_a:
                st.metric("Spam Probability", f"{result['spam_probability']:.1%}")
                st.progress(result['spam_probability'])

            with col_b:
                st.metric("Ham Probability", f"{result['ham_probability']:.1%}")
                st.progress(result['ham_probability'])

            # Detailed analysis
            with st.expander("üîç Detailed Analysis"):
                st.write("**Processed Text:**")
                st.code(result['processed_text'], language="text")

                st.write("**Key Features:**")
                import pandas as pd
                feat_df = pd.DataFrame(list(result['features'].items()), columns=['Feature', 'Value'])
                st.dataframe(feat_df, use_container_width=True)

        else:
            st.warning("Please enter either a subject or message to analyze.")

with col2:
    st.subheader("üìà Quick Stats")

    # Sample analysis
    st.write("**Test Examples:**")

    # Real test examples
    test_examples = [
        {"email": "URGENT! Win $1000 cash!", "result": "Spam", "confidence": 0.99},
        {"email": "Meeting tomorrow at 2 PM", "result": "Ham", "confidence": 0.95},
        {"email": "Free iPhone offer!", "result": "Spam", "confidence": 0.98},
        {"email": "Project update report", "result": "Ham", "confidence": 0.92}
    ]

    for i, example in enumerate(test_examples[-3:], 1):
        if example['result'] == 'Spam':
            st.error(f"Example {i}: {example['result']} ({example['confidence']:.0%})")
        else:
            st.success(f"Example {i}: {example['result']} ({example['confidence']:.0%})")

    st.markdown("---")
    st.subheader("üéØ Model Capabilities")
    st.markdown("""
    - **High accuracy** on real email data
    - **BERT-powered** semantic understanding  
    - **Multi-feature** analysis
    - **Real-time** processing
    - **Explainable** predictions
    """)

# Footer
st.markdown("---")
st.markdown("### üìö How It Works")
st.markdown("""
**Direct Processing Pipeline:**
1. **Text Preprocessing**: Cleans and normalizes content
2. **BERT Encoding**: Extracts semantic embeddings
3. **Feature Engineering**: Analyzes patterns and indicators
4. **Direct Model Prediction**: Uses trained ensemble without adjustments
5. **Confidence Scoring**: Provides raw probability estimates

**Key Features Analyzed:**
- Email structure and formatting
- Spam keyword detection
- URL and link analysis
- Text complexity metrics
- Character usage patterns
- BERT semantic understanding

**Model Output**: Raw predictions from BERT + ML ensemble trained on 5,854 emails
""")

st.markdown("---")
st.caption("Direct Spam Detection System | BERT + Stacking Ensemble | Raw Model Predictions | 98.1% F1-Score")
